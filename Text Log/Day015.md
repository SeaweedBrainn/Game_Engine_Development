# Day 15
### Jan 11, 2026

[Link to current edition of the game engine repository](https://github.com/SeaweedBrainn/GameEngineProject/tree/18d1e10bb68f67e418fae23c846cf913f394093f)

I created the camera class today, obj mesh loading, EBOs and implemented the rotation, scaling, and perspective camera transformations (creating MVP matrix multiplication). I continued following [BennyBox's tutorial playlist](https://youtube.com/playlist?list=PLEETnX-uPtBXP_B2yupUKlflXBznWIlL5&si=IXih23VENDWYb3b4) and completed till the #20 video.

<hr />

> ### Resource Loader Class and refactoring Shader class
I created a resource loader class thats gonna import external data from the files on the system. I refactored the Shader code to use a method from the Resource Loader Class that reads a shader file code into a `std::string`.

<hr />

> ### .obj file parsing implementation
I implemented a simple .obj file parser to get the mesh data from an obj file. For now I am assuming that the mesh data is just vertices and [indices](#using-ebo-with-mesh-class). To get such a file from blender you enable the `triangulate faces` when exporting the model. As such you get data in the form `v x y z` and `f a b c`, where x,y, and z represent the vertex coordinates in model space and a,b,c represent the indices that which vertex is connected to which vertex to make a triangle. Note to be made that obj file indices are 1 based while [openGL element object buffer indices](#using-ebo-with-mesh-class) are 0 based, so the conversion had to be taken in consideration. I took the data and created a mesh from it. <br />

A few things I learnt from this exercise were about the various iostream objects in C++. A `stringstream` object is a string which can be used for I/O operations like `<<` and `>>`. To use it you need to import the `<sstream>` library. An `istringstream` object is similar but you can only do input operations like `>>` on it. My logic here involved getting each line of the file, if it started with a v, then store the next three values in 3 variables and attach the vertex coordinated. Then doing the same for the indices.

<hr />

> ### Using EBO with mesh class
Currently my mesh data was just vertices, so if I had to render more than one triangle, there would be a lot of repetition of vertices. As a result we use an EBO (element buffer object) to hold indices that basically indicate faces of the mesh to tell which vertex connects to which vertex. The details to the entire procedure in given in the [learn OpenGL website](https://learnopengl.com/Getting-started/Hello-Triangle), it is again very straightforward. I used EBOs and refactored my mesh class to take in faces and vertices.

<hr />

> ### Camera Class
I created a camera class which holds the current position of camera, the relative upward direction (y axis), the relative forward direction (-z axis), and the global y axis. The class also has methods to move the camera's position depending on a movement vector, and rotating it about the x or y axis. The rotation was handled by finding the relative x axis using vector products, rotating the forward vector about the respective axis using a method created in the vector3f class, then updating the upward vector by using vector cross products again. We rotate about the local x axis (pitch) and the global y axis (yaw). I also created an input method to move the camera by WASD and the arrow keys. <br />

Rotation of a vector3f around any specified axis (a normalized vector) can be done using quaternions. [This paper: Rotation Quaternions, and How to Use Them, May, 2015](https://danceswithcode.net/engineeringnotes/quaternions/quaternions.html) by D. Rose is a great guide on quaternion rotation. I did not use all of its knowledge for rotating the vertex as i still have the gimbal lock problem but for the current purposes it supplies enough instructions. I converted the axis into a quaternion q using axis-angle to quaternion conversion formula and performed the q * p * q^(-1) calculation to calculate the newly rotated vector. <br />

Note: At this instance I found out that I had made a typo in the quaternion multiplication formula so I fixed that as well.

<hr /> 

> ### Enhancing the transform class
I had to get all the transformation matrices from the transform class so I could actually use the camera and do other stuff with the mesh. I completed the rotation and scale matrices in the Matrix4f class, which along with the translation matrix make up the Model Matrix `(T * R * S)`. I set a function to output the Model Matrix, this matrix converts the vertices from object space to world space. Now, the current project provided is enough to be able to move the mesh around, rotate it, and scale it. By default the midpoint of the screen will be (0,0,0) and the view will be orthographic, the camera doesnt work as of yet. If the object is not visible just scale it down. Also its possible the object is inside out so nothing is visible, so change the glFrontFace() in the render utils class. The method getTransformation() now returns the Model Matrix. <br />

Next, I created the View matrix or the Camera matrix, which uses the camera to transform the vertices from world space to view space relative to the camera. You can just google for the formula for this matrix once again but what I did was different than the usual stuff, I just used the translation matrix I created earlier to handle camera translation and created a new Matrix4f method to create the camera rotation matrix, which uses the local axes of the camera to see which way its looking. Note: by default, camera is assumed to be at (0,0,0) and we are assumed to be looking towards the -z axis. (Yeah I don't know a lots of graphics pipeline theory goes in this, i need to read up on that). If we used this matrix along with the model matrix to show stuff on the screen with our basic arrangement, we would be able to use the camera but the projection would be orthographic and the movement of the camera make the object behave very wonkily. <br />

Finally, i created the Projection matrix to transform the object from View Space to Clip Space. Its this clip space which defines the confines of the camera, and how much of the objects in the scene is rendered and how much is clipped. Again the matrix can be googled, I made a perspective projection. Its also a bit complicated to explain so again we need to go into the graphics pipeline to understand, but it works. The combination of the Model, View, and Projection matrices is called as the MVP matrix and the order of multiplication is `P * V * M`. You can view [this website](https://jsantell.com/model-view-projection/) to learn a bit about the MVP matrix. The MVP matrix is multiplied with the vertex location in the vertex shader to be finally able to see the objects. However, note that now that the camera is at (0,0,0), the object also at (0,0,0) wont be visible or would be clipped and you'd also see some artifacts of the perspective view. To fix it, use the translate method of the transform class to translate the objects a few units down the -z direction and you'd be able to see it clearly.

**BIG NOTE (DON'T SKIP):** In the project I realized I have set the +z as the forward of the camera and the view matrix, as such the matrix I am using doesn't match the one you'd google exactly. That is just due to the fact that I am using a youtube playlist to make the engine, the theory i am looking up and reading on my own. The conventional method would be taking -z as the direction your camera's forward faces.

> ### Using the shader to perform the transformation
After obtaining the MVP transformation matrix, we also need to apply it to the vertex locations. For this modify the vertex shader to have a `uniform mat4 transform`. Then update this mat4 every second. To set a mat4vf uniform (check the shader class), you need to pass a 1D array of all the entries. The third argument needs to be `GL_TRUE` since our matrix is stored in a row-major order and openGL naturally expects column-major order, so it transposes the data itself after receiving it by `GL_TRUE`. I hadn't explained this well enough in yesterday's log. Now multiply the transform with the vertex. The final vertex position is always `P * V * M * v` where v is the vertex coordinate. 
